{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<center>\n    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **Weather Classification Assignment**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Estimated time needed: **60** minutes\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this notebook, we will practice using all the classification algorithms and metrics that we learned in this course. Using weather data we will try to predict if there is going to rain the next day.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "After completing this lab you will be able to:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*   Data\n    *   Describe and Define the Dataset\n    *   Load a CSV Dataset using Pandas\n    *   Preprocess the Data using Pandas\n    *   Deal with NULL Values in your Dataset\n    *   Perform One Hot Encoding on Categorical Variables\n    *   Split your Data into a Training and Testing Set\n    *   Standardize your Data using StandardScaler or MinMax\n*   Classification\n    *   Use GridSearchCV to Find the Best Parameters for a Classification Algorithm\n    *   Perform Classification using Logistic Regression\n    *   Perform Classification using K-Nearest Neighbors\n    *   Perform Classification using Support Vector Machine\n    *   Perform Classification using Decision Trees\n*   Use Evaluation Metrics Accuracy Score, Jaccard Index, F1-Score, and Log Loss on Each Algorithm and Report the Results\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Setup\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "First, we will download the data that we will use in this lab which is stored in a CSV format.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from js import fetch\nimport io\n\nURL = \"https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/ML0101ENv3/project_EdX/weatherAUS.csv\"\nresp = await fetch(URL)\ntext = io.BytesIO((await resp.arrayBuffer()).to_py())",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we are going to be using Python and several Python libraries.\n\nIf you are running this Jupyter Notebook locally, you need to install the following libraries by uncommenting the code bellow. Otherwise, leave the code bellow commented out and run the rest of this notebook.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#pip install pandas\n#pip install sklearn\n#pip install matplotlib\n#pip install numpy",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# allows us to interact with the data using a dataframe\nimport pandas as pd\n# allows us to interact with the data and perform calculations using ndarrays\nimport numpy as np\n# various classification algorithms and metrics from sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n# matplotlib allows us to create graphs\nimport matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Since sklearn calculates jaccard index differently than what was taught in the course we will define our own function for jaccard index\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# works like sklearn classificaton metrics given list or ndarray of predictions and values returns the jaccar index\ndef jaccard_index(predictions, true):\n    if (len(predictions) == len(true)):\n        intersect = 0;\n        for x,y in zip(predictions, true):\n            if (x == y):\n                intersect += 1\n        return intersect / (len(predictions) + len(true) - intersect)\n    else:\n        return -1",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### About the Data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The original source of the data is Austrailian Government's Bureau of Meteorology and the latest data can be gathered from [http://www.bom.gov.au/climate/dwo/](http://www.bom.gov.au/climate/dwo/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01).\n\nThe dataset we will use has extra columns like RainToday and our target RainTomorrow which was gathered from Rattle at [https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData](https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This dataset is observations of weather metrics for each day from 2008 to 2017. The **weatherAUS.csv** dataset includes the following fields:\n\n| Field         | Description                                           | Unit            | Type   |\n| ------------- | ----------------------------------------------------- | --------------- | ------ |\n| Date          | Date of the Observation in YYYY-MM-DD                 | Date            | object |\n| Location      | Location of the Observation                           | Location        | object |\n| MinTemp       | Minimum temperature                                   | Celsius         | float  |\n| MaxTemp       | Maximum temperature                                   | Celsius         | float  |\n| Rainfall      | Amount of rainfall                                    | Millimeters     | float  |\n| Evaporation   | Amount of evaporation                                 | Millimeters     | float  |\n| Sunshine      | Amount of bright sunshine                             | hours           | float  |\n| WindGustDir   | Direction of the strongest gust                       | Compass Points  | object |\n| WindGustSpeed | Speed of the strongest gust                           | Kilometers/Hour | object |\n| WindDir9am    | Wind direction averaged 10 minutes prior to 9am       | Compass Points  | object |\n| WindDir3pm    | Wind direction averaged 10 minutes prior to 3pm       | Compass Points  | object |\n| WindSpeed9am  | Wind speed averaged 10 minutes prior to 9am           | Kilometers/Hour | float  |\n| WindSpeed3pm  | Wind speed averaged 10 minutes prior to 3pm           | Kilometers/Hour | float  |\n| Humidity9am   | Humidity at 9am                                       | Percent         | float  |\n| Humidity3pm   | Humidity at 3pm                                       | Percent         | float  |\n| Pressure9am   | Atmospheric pressure reduced to mean sea level at 9am | Hectopascal     | float  |\n| Pressure3pm   | Atmospheric pressure reduced to mean sea level at 3pm | Hectopascal     | float  |\n| Cloud9am      | Fraction of the sky obscured by cloud at 9am          | Eights          | float  |\n| Cloud3pm      | Fraction of the sky obscured by cloud at 3pm          | Eights          | float  |\n| Temp9am       | Temperature at 9am                                    | Celsius         | float  |\n| Temp3pm       | Temperature at 3pm                                    | Celsius         | float  |\n| RainToday     | If there was rain today                               | Yes/No          | object |\n| RISK_MM       | Amount of rain tomorrow                               | Millimeters     | float  |\n| RainTomorrow  | If there is rain tomorrow                             | Yes/No          | float  |\n\nColumn definitions were gathered from [http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml](http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Load the Dataset\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets use the **head()** function to see our data\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(text)\n\ndf.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Preprocessing\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We want to focus specifically on Sydney so that we can train our algorithm quickly. You can select other locations or multiple locations if you would like to experiment.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df[df['Location'] == 'Sydney']",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, we drop all the columns in the table that we won't need.\n\nWe drop Location because it is constant for each row and we drop RIS_MM because this tells us the amount of rain tomorrow so we can not train on it as it reveals the target and we are doing classification, not regression.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney = df[df['Location'] == 'Sydney']\n\ndf_sydney.drop(columns=['Location', 'RISK_MM'], axis=1, inplace=True)\n\nprint(df_sydney.shape)\n\ndf_sydney.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As you can see above we have NaN occur a couple of times in our dataset. We can either drop the data or replace the data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Below we can see how many NaN values we have for each row. WindGustDir, WindGustSpeed, Cloud9am, and Cloud3pm have large values of missing data. In this case for \\~33% of the data, we are missing a value for WindGusDir and WindGustSpeed. This is not enough to remove the entire column but we will perform some preprocessing.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney.isna().sum()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Dealing With Nulls\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Please uncomment the method that you would like to use\n\n1.  Drop all rows that contain NaN\n2.  Replace NaN in object type columns like WindGustDir with most frequent value in the column and replace NaN in float type columns like WindGustSpeed, Cloud9am, and Cloud3pm with the mean. Then we drop the remaining rows with NaN in them.\n\nPlease note that if you choose to replace the NaN values the classification algorithms will take a little longer to compute\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1.  Drop\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_filled = df_sydney.dropna()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "2.  Replace\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_filled = df_sydney.copy()\n\nmost_frequent_WindGustDir = df_sydney_filled['WindGustDir'].value_counts().idxmax()\ndf_sydney_filled[\"WindGustDir\"].replace(np.nan, most_frequent_WindGustDir, inplace=True)\n\nmean_WindGustSpeed = df_sydney_filled[\"WindGustSpeed\"].astype(\"float\").mean(axis=0)\ndf_sydney_filled[\"WindGustSpeed\"].replace(np.nan, mean_WindGustSpeed, inplace=True)\n\nmean_Cloud9am = df_sydney_filled[\"Cloud9am\"].astype(\"float\").mean(axis=0)\ndf_sydney_filled[\"Cloud9am\"].replace(np.nan, mean_Cloud9am, inplace=True)\n\nmean_Cloud3pm = df_sydney_filled[\"Cloud3pm\"].astype(\"float\").mean(axis=0)\ndf_sydney_filled[\"Cloud3pm\"].replace(np.nan, mean_Cloud3pm, inplace=True)\n\ndf_sydney_filled.dropna(inplace=True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(df_sydney_filled.shape)\ndf_sydney_filled.isna().sum()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As you can see we have completely removed all NaN values using different methods which allow you to either remove rows with NaN in them improving the pureness of our dataset or filling in NaN values allowing us to preserve rows. When deciding on the method to use there are many benefits and drawbacks we must consider like whether or not we will have enough data after dropping NaN rows or if filling in Nan by frequency or mean will introduce some sort of bias to our data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_filled.loc[:,'Date'] = df['Date'].str.replace('-', '')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Finally, we remove the - between the values of the Date column so they can be converted to floats\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### One Hot Encoding\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Finally we need to perform one hot encoding to convert categorical variables to binary variables\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_processed = pd.get_dummies(data=df_sydney_filled, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, we replace the values of the RainTomorrow column changing it from a categorical column to a binary column. We do not use the **get_dummies** method because we would end up with two columns for RainTomorrow and we do not want that because it is our target.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_processed.replace(['No', 'Yes'], [0,1], inplace=True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Training Data and Testing Data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "First, we turn all columns into a float type. We don't need to do this because the **StandardScalar()** method will convert object types to float but it will give us a warning message.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sydney_processed = df_sydney_processed.astype(float)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we split our dataset into a features dataset and target dataset. We drop our target to create our features dataset and only keep RainTomorrow to create our target dataset\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features = df_sydney_processed.drop(columns='RainTomorrow', axis=1)\nY = df_sydney_processed['RainTomorrow']",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we will standardize the data. We can do this in multiple ways like using the **StandardScalar()** method which will scale the values to unit variance or the **MinMaxScalar()** which will scale each value to the min and max of each column.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Data Standardization\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Before we standardize our data we must split it into training and testing sets. We do this before standarsizing so that we don't give any hints to out model by standardizing all the data together.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=.2, random_state=1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Please uncomment the method you would like to choose\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "norm = preprocessing.StandardScaler()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "2.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "norm = preprocessing.MinMaxScaler()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x_train = norm.fit_transform(x_train)\n\nx_test = norm.transform(x_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we discussed before you can see how we fit and the scaler to the training data and also transformed it. Then we used the fitted scaler to transform the test data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Classification\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Instructions\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Below is where we are going to use the classification algorithms to create a model based on our training data and finally evaluate our testing data using evaluation metrics learned in the course\n\nWe will some of the algorithms taught in the course, specifically\n\n1.  Logistic Regression\n2.  KNN\n3.  SVM\n4.  Decision Trees\n\nWe will evaluate our models using\n\n1.  Accuracy Score\n2.  Jaccard Index\n3.  F1-Score\n4.  Log Loss\n\nNote: Jaccard Index is calculated differently in Sci Kit Learn so I have defined a function at the top of the notebook for you to use, its input style is the same as Sci Kit Learn\n\nAs we know these algorithms have many parameters and to find the best ones we will use GridSearchCV\n\nI will demonstrate how to do this using a mock classification algorithm\n\n1.  Create a python dictionary with the key being the name of the parameters and the value being a list of possible values\n2.  Create an object of the classification algorithm\n3.  Create a GridSearchCV object and place your classification object and parameters dictionary as parameters, also define your GridSearchCV cv parameter (Use cv = 4)\n4.  Use the fit method of the GridSearchCV algorithm to train our model using x_train and y_train that we create before\n5.  Store the best model in a variable provided\n6.  Predict the target variable using the x_test data we created above\n7.  Calculate and store the values for each metric in the provided variables using the predictions and y_test data\n\nYou will need to research the parameters you need to use as there are many options but this is simple. GridSearchCV will determine the best model.\n\nFinally using your models generate the report at the bottom\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Mock\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "parameters = {'C': [.001, .01, .1, 1, 10, 100],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\nMock = MockClassification()\n\nGrid = GridSearchCV(Mock, parameters, cv = 4)\n\nGrid.fit(x_train, y_train)\n\nBestMock = Grid.best_estimator_\n\npredictions = BestMock.predict(x_test)\n\nBestMock_Accuracy_Score = accuracy_score(predictions, y_test)\nBestMock_JaccardIndex = jaccard_index(predictions, y_test)\nBestMock_F1_Score = f1_score(predictions, y_test)\nBestMock_Log_Loss = log_loss(y_test,BestLR.predict_proba(x_test)[:, 1])",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If you need some more help with grid search here are a couple of resources\n\n1.  [https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n2.  [https://scikit-learn.org/stable/modules/grid_search.html](https://scikit-learn.org/stable/modules/grid_search.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Question 1: Logistic Regression\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For Logistic Regression please use the parameters C = \\[.001, .01, .1, 1, 10, 100] and solver. Use the link provided to select the values for the solver parameter. [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "When creating the LogisticRegression object please make **max_iter = 10000**. This will allow us enough iteration so the model parameters can converge\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "BestLR = ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(BestLR)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "LR_Accuracy_Score = \nLR_JaccardIndex = \nLR_F1_Score = \nLR_Log_Loss = ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Question 2: KNN\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For KNN please use the parameters n_neighbors = \\[1,2,3,4,5,6,7,8,9,10], algorithm, and p. Use the link provided to select the values for algorithm and p. [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "BestKNN =",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(BestKNN)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "KNN_Accuracy_Score = \nKNN_JaccardIndex = \nKNN_F1_Score = ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Question 3: SVM\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For SVM please use the parameters C = \\[.001, .01, .1, 1, 10, 100] and kernel. Use the link provided to select the values for kernel. [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "BestSVM =",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(BestSVM)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "SVM_Accuracy_Score = \nSVM_JaccardIndex = \nSVM_F1_Score = ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Question 4: Decision Tree\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For Decision Tree please use the parameters criterion. Use the link provided to select the values for criterion. [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "BestTree =",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(BestTree)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Tree_Accuracy_Score = \nTree_JaccardIndex = \nTree_F1_Score = ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Report\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Report = pd.DataFrame({'Algorithm' : ['KNN', 'Decision Tree', 'SVM', 'LogisticRegression']})\n\nReport['Accuracy'] = [LR_Accuracy_Score, KNN_Accuracy_Score, SVM_Accuracy_Score, Tree_Accuracy_Score]\nReport['Jaccard'] = [LR_JaccardIndex, KNN_JaccardIndex, SVM_JaccardIndex, Tree_JaccardIndex]\nReport['F1-Score'] = [LR_F1_Score, KNN_F1_Score, SVM_F1_Score, Tree_F1_Score]\nReport['LogLoss'] = ['N/A', 'N/A', 'N/A', LR_Log_Loss]\n\nReport",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Azim Hirjani](https://www.linkedin.com/in/azim-hirjani-691a07179/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "| Date (YYYY-MM-DD) | Version | Changed By | Change Description         |\n| ----------------- | ------- | ---------- | -------------------------- |\n| 2020-09-14        | 0.2     | Azim       | Update Lab to Use Template |\n| 2020-04-17        | 0.1     | Azim       | Created Lab                |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright © 2020 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}